{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying Wikipedia HTML Dumps\n",
    "This notebook provides a tutorial for how to work with the [Wikimedia Enterprise](https://meta.wikimedia.org/wiki/Wikimedia_Enterprise) HTML dumps. The format of the dumps is detailed in this [README](https://dumps.wikimedia.org/other/enterprise_html/). An introduction to some of the crucial ways in which the parsed HTML version of Wikipedia articles can differ from the raw [wikitext](https://en.wikipedia.org/wiki/Help:Wikitext) that editors use to construct articles can be found in [Mitrevski et al.](https://arxiv.org/pdf/2001.10256.pdf)\n",
    "\n",
    "This notebook has four stages:\n",
    "* Accessing the HTML dumps\n",
    "* Working with wikitext\n",
    "* Working with HTML\n",
    "* Comparing the HTML and wikitext versions of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the notebook \n",
    "\n",
    "The given notebook has four sections, namely, \n",
    "- Accessing the HTML dumps\n",
    "- Working with wikitext \n",
    "- Working with HTML \n",
    "- Comparing the HTML and wikitext versions of articles \n",
    "\n",
    "Working with wikitext involved the use of `mwparserfromhell` and to work with HTML, `BeautifulSoup` was significantly used. Other libraries of python such as `nltk`, `matplotlib`, along with BeautifulSoup have also been used to work with the comparison aspect of the notebook.  \n",
    "  \n",
    "  HTML and wikitext have been compared on three levels which are the following : \n",
    "  - Feature Analysis\n",
    "  - Text Analysis\n",
    "  - Time Analysis\n",
    "  \n",
    "Please note : Comments have been added wherever necessary to make it convenient for the user to understand what is being done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the HTML dumps\n",
    "This is an example of how to parse through the HTML dumps for a wiki -- i.e. complete parsed HTML versions of all the articles in a given project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # format HTML object is stored in\n",
    "import tarfile  # necessary for decompressing dump file into text format\n",
    "import os  # file path manipulation\n",
    "\n",
    "import mwparserfromhell as mw  # best library for working with the raw wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML is available for every Wikimedia project: https://www.mediawiki.org/wiki/Special:SiteMatrix\n",
    "# The format here is the language code (en = English; ar = Arabic; etc.)\n",
    "# followed by the project code (wiki = Wikipedia; wikisource = Wikisource)\n",
    "SITENAME = 'enwiki'\n",
    "\n",
    "# Dump date -- only the last 6 runs are kept so this dump may at one point no longer be available and need updated\n",
    "DATE = '20220201'\n",
    "\n",
    "# namespace code -- namespace 0 is the article namespace. Other namespaces contain Media files or talk pages etc.\n",
    "# See: https://en.wikipedia.org/wiki/Wikipedia:Namespace\n",
    "NS = 0\n",
    "\n",
    "# directory on PAWS server that holds Wikimedia dumps\n",
    "DUMP_DIR = \"/public/dumps/public/other/enterprise_html/runs\"\n",
    "HTML_DUMP_FN = os.path.join(DUMP_DIR, DATE, f'{SITENAME}-NS{NS}-{DATE}-ENTERPRISE-HTML.json.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/public/dumps/public/other/enterprise_html/runs/20220201/enwiki-NS0-20220201-ENTERPRISE-HTML.json.tar.gz': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Very large dataset; we will only work with a small portion\n",
    "# Choosing a different language would make processing more manageable though\n",
    "!ls -shH \"{HTML_DUMP_FN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(HTML_DUMP_FN, mode=\"r:gz\") as tar:\n",
    "    # we're just going to look at the first block of articles -- each one is a maximum of 10GB\n",
    "    html_fn = tar.next()\n",
    "    print(f'We will be working with {html_fn.name} ({html_fn.size / 1000000000:0.3f} GB).')\n",
    "    # extract the first file\n",
    "    with tar.extractfile(html_fn) as fin:\n",
    "        for line in fin:\n",
    "            article = json.loads(line)\n",
    "            break\n",
    "\n",
    "# As you can see, each line not only contains the HTML of the article (`html`),\n",
    "# it also contains the raw wikitext (`wikitext`) that was parsed to generate that HTML\n",
    "# and a lot of article metadata such as the categories, templates, and whether there are any redirects (https://en.wikipedia.org/wiki/Wikipedia:Redirect)\n",
    "# and metadata about the most recent edit (date, comment left by the editor, username, etc.)\n",
    "# https://en.wikipedia.org/wiki/Chang_Gum-chol\n",
    "print(f\"\\nExample article in dump: {json.dumps(article, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with wikitext\n",
    "Because the wikitext dumps have been available for a long time, there is a rich ecosystem of tools for manipulating and parsing it. A great example of this [mwparserfromhell](https://pypi.org/project/mwparserfromhell/), which is a Python library that can be used to parse wikitext into recognizable objects such as [links](https://en.wikipedia.org/wiki/Help:Link), [templates](https://en.wikipedia.org/wiki/Help:Template), or [sections](https://en.wikipedia.org/wiki/Help:Section).\n",
    "\n",
    "Here we will show all of the nice things you can do with mwparserfromhell and the wikitext from our example article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw wikitext -- you can see some of the article text but also some weird template code and all sorts of syntax\n",
    "article['article_body']['wikitext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = mw.parse(article['article_body']['wikitext'])\n",
    "\n",
    "# types of objects that can be extracted from the wikitext\n",
    "# note that `html_entities` is not the parsed HTML but text like `&Sigma;` that is code for Î£\n",
    "[f for f in dir(wt) if f.startswith('filter')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This article has:')\n",
    "print(f'* {len(wt.get_sections())} sections ({\", \".join([str(h.title) for h in wt.filter_headings()[:2]])}, ...)')\n",
    "print(f'* {len(wt.filter_wikilinks())} wikilinks ({\", \".join([str(l.title) for l in wt.filter_wikilinks()[:2]])}, ...) '\n",
    "      f'including {len(wt.filter_wikilinks(matches=lambda c: c.title.startswith(\"Category:\")))} categories')\n",
    "print(f'* {len(wt.filter_templates())} templates ({\", \".join([str(t.name) for t in wt.filter_templates()[:2]])}, ...)')\n",
    "print(f'* {len(wt.filter_external_links())} external links ({\", \".join([str(e.url) for e in wt.filter_external_links()[:1]])}, ...)')\n",
    "print(f'* {len(wt.filter_tags())} tags ({\", \".join([str(t.tag) for t in wt.filter_tags()[:5]])}, ...) including {len(wt.filter_tags(matches=lambda t: t.tag == \"ref\"))} reference.')\n",
    "print(f'* {len(wt.filter_comments())} comment ({wt.filter_comments()[0]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not perfect, but we can also remove all the wikitext syntax and just leave behind the article text:\n",
    "print(wt.strip_code())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with HTML\n",
    "\n",
    "Here you will show how you might extract similar features from the HTML. Complete the TODOs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll almost certainly want to take advantage of some existing Python libraries for working with the HTML\n",
    "# such as BeautifulSoup (https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from bs4 import Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article['article_body']['html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : Different Feature Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose a few of the features above -- e.g., sections, categories -- or any others you can think of\n",
    "# Show how to extract them from the HTML\n",
    "# If you get different counts than the counts based on the wikitext, see if you can explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(article['article_body']['html'])\n",
    "\n",
    "#find all the tags\n",
    "for tags in soup.findAll(True):\n",
    "    print(tags.name)\n",
    "    \n",
    "#find all the external links\n",
    "for a in soup.find_all('a', href=True):\n",
    "    print(a['href'])\n",
    "\n",
    "#find all comments\n",
    "comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "for c in comments:\n",
    "    print(c)\n",
    "    \n",
    "#total number of sections\n",
    "for s in soup.find_all('sections'):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This article has:')\n",
    "\n",
    "#total number of tags\n",
    "print('Total number of tags is', len(soup.findAll(True)))\n",
    "\n",
    "#total number of external links\n",
    "print('Total number of external links is', len(soup.find_all('a', href=True)))\n",
    "\n",
    "#total number of comments\n",
    "print('Total number of comments is', len(soup.find_all(string=lambda text: isinstance(text, Comment))))\n",
    "\n",
    "#total number of sections\n",
    "print('Total number of sections is', len(soup.find_all(\"sections\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different counts of features\n",
    "\n",
    "### The above cell extracted four features from HTML, namely, tags, external links and comments. \n",
    "- The total number of tags extracted by wikitext is 23 whereas by HTML is 173.\n",
    "- The total number of external links extracted by wikitext is 2 whereas by HTML is 24.\n",
    "- The total number of comments extracted by both wikitext and HTML is 1.\n",
    "- The total number of sections extracted by wikitext is 5 whereas by HTML is 0.\n",
    "\n",
    "The MediaWiki software is used in parsing wikitext to HTML. As mentioned in 'WikiHist.html: English Wikipedia's Full Revision History in HTML Format' (Mitrevski, Piccardi, & West, 2020), during parsing, additional macros (templates and modules) are added in HTML which in turn provide more information implicit in the external templates and modules. These external macros are invoked by wikitext. Due to this, there is more information present in HTML than wikitext and wikitext just becomes an approximation.  \n",
    "These authors also conducted an analysis on the hyperlinks and found that over half of the wikilinks present in HTML are missing from the raw wikitext (Mitrevski, Piccardi, & West, 2020).  \n",
    "This analysis is also proven from the statistics received by us. Working with one another and extracting features from wikitext and HTML, we obtained the above mentioned results. The links extracted from wikitext equal 2 whereas the number extracted by HTML equals 24 which confirms that over half of the wiki links pres\n",
    "Furthermore, this software only allows the use of a certain set of HTML elements. The rest are deprecated HTML elements and need be used with some replacement. The following sections talk more about deprecated HTML elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Are there features / data that are available in the HTML but not the wikitext?\n",
    "# You may have to go through several example articles to find these\n",
    "# https://www.mediawiki.org/wiki/Specs/HTML/2.4.0 has some additional details about what you might find\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HTML v/s wikitext\n",
    "\n",
    "### Data \n",
    "\n",
    "The MediaWiki software parses wikitext to HTML. During parsing, additional content is added through the process of expanding the templates and the modules. This expansion of templates results in significant portions of HTML being inserted in the parsed output. Therefore, the presence of these macros (templates and modules) in HTML implies there is more information provided by HTML than the wikitext source. \n",
    "  \n",
    "  Wikitext can only provide an approximation of the complete information available in its corresponding Wikipedia article, the cause of this, as already mentioned, is because of the parsing process. During this process, the expansion of templates and modules provides more information implicit in external macros which are invoked by the wikitext. Thus, while studying wikitext one must remember that the information offered through wikitext is only an approximation of the complete information from the articles (Mitrevski, Piccardi, & West, 2020). \n",
    "  \n",
    "  For example : In an analysis conducted on Wikipedia's hyperlinks, it was observed that over half of the wiki links present in HTML are missing from raw wikitext (Mitrevski, Piccardi, & West, 2020).\n",
    "  \n",
    "  Hyperlinks is one such example to clearly identify the differences in data in html and wikitext. The macros (templates and modules) invoked by wikitext during the process of parsing also includes other entities such as tables, images, references, tags and more. Below, tags have been used as an example to explain in detail the discrepancies noticed in html and wikitext. \n",
    "\n",
    "### Tags\n",
    "Some HTML tags have an equivalent in wikitext markup and hence are generally preferred whereas there are a few deprecated HTML tags which may not be supported in the future. Thus, in order to use these deprecated HTML tags, one requires a replacement for these tags. \n",
    "  \n",
    "  For Example : \n",
    "\n",
    "HTML Deprecated tag : `<big>`\n",
    "  \n",
    "  Replacement : `<span style=\"font-size: larger;\">`\n",
    "\n",
    "HTML Deprecated tag : `<tt>`\n",
    "  \n",
    "  Replacement : `<code>` for source code ; `<kbd>` for user input ; `<var>` for variables and `<samp>` for computer output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function for extracting the article text\n",
    "# It doesn't have to look the same as the output of `wt.strip_code()` above (in fact, it likely won't)\n",
    "# but it should be very similar in that you should aim for something\n",
    "# that captures the text of the article without a lot of markup etc.\n",
    "# NOTE: straightforward HTML -> text functions likely won't perform well here and you'll probably\n",
    "# want to write something more custom to handle the specifics of Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following section, I have used two methods. \n",
    "- Method 1 : Pre-processing of data\n",
    "- Method 2 : Using BeautifulSoup\n",
    "\n",
    "### Method 1 : Pre-processing of data \n",
    "\n",
    "For this method, I have used `regex` and the following five functions have been created.\n",
    "\n",
    "- remove_punct : This function removes all the punctuations from the text. \n",
    "- remove_urls : This function removes all the URLs. The URLs are identified with `https://` and `www.`\n",
    "- remove_tag : This function removes all the tags from the HTML and have been identified with angular brackets, that is, `<` and `>`\n",
    "- pre_process_sentence : This function uses the above mentioned two functions namely `remove_punct` and `remove_urls`. It takes the sentences as itâs argument and converts all sentences into lowercase. \n",
    "- pre_process_article : This function uses `pre_process_sentence` function and sends the article as its argument. The `sent_tokenizer` splits the sentences into tokens and perfoms tokenization. The processed sentences are then appended to the sentences list and this `sentences` list is then returned. \n",
    "\n",
    "The extracted text is returned in `extracted_text`. \n",
    "\n",
    "Method 1 was utilized as a way for pre-processing but it has one major limitation. The function that removes the tags only removes the angular brackets leaving the words in the text. Thus, for this notebook, I would be using Method 2.  \n",
    "I have also used other methods like `lxml`, `regex` and `unescape` but the results have not been satisfactory and thus I have omitted their use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "html_article = BS(article['article_body']['html'])\n",
    "\n",
    "def remove_punct(text):\n",
    "  text = re.sub('[^a-zA-Z0-9 ]+','', text)\n",
    "  return text\n",
    "\n",
    "def remove_urls(text):\n",
    "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "  return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_tag(text):   \n",
    "  text=' '.join(text)\n",
    "  html_pattern = re.compile('<.*?>')\n",
    "  return html_pattern.sub(r'', text)\n",
    "\n",
    "def pre_process_sentence(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = remove_punct(remove_urls(sentence))\n",
    "  return sentence\n",
    "\n",
    "def pre_process_article(article):\n",
    "    article = str(article).replace(\"\\n\", '')\n",
    "    article = sent_tokenize(article)\n",
    "    sentences = []\n",
    "    for each in article:\n",
    "        if len(each.split(\":\")) <= 1:\n",
    "            continue\n",
    "        sentences.append(pre_process_sentence(each))\n",
    "    return sentences\n",
    "\n",
    "extracted_text = pre_process_article(html_article)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import html as ihtml\n",
    "import pandas as pd\n",
    "\n",
    "text = BS(article['article_body']['html'])\n",
    "print(soup.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikitext vs. HTML comparison\n",
    "Use the methods you developed above to do a comparison of the similarity between the wikitext and parsed HTML for a larger sample of at least 100 articles. Document your results in a mixture of data, graphs/charts/figures, and markdown explaining the differences that you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the comparison, the following four function are used for :\n",
    "- `output_wikitext` and `output_html` return the four features, namely, tags, external links, comments and sections. \n",
    "- The `wikitext` and `html` functions use article has their argument and send it to output_wikitext and output_html respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = []\n",
    "\n",
    "def output_wikitext(wt):\n",
    "    return [len(wt.filter_tags()), len(wt.filter_external_links()), len(wt.filter_comments()), len(wt.get_sections())]\n",
    "\n",
    "def output_html(soup):\n",
    "    return [len(soup.findAll(True)), len(soup.find_all('a', href=True)), len(soup.find_all(string=lambda text: isinstance(text, Comment))), len(soup.find_all(\"sections\"))]\n",
    "    \n",
    "def wikitext(article):\n",
    "    wt = mw.parse(article)\n",
    "    [f for f in dir(wt) if f.startswith('filter')]\n",
    "    a = output_wikitext(wt)\n",
    "    return a\n",
    "    \n",
    "def html(article):\n",
    "    return output_html(BS(article))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: feature comparison analysis\n",
    "# For example, how does the average number of links vary between the two? Do your best to explain the discrepancies.\n",
    "# If possible, relate back to Mitrevski et al. (https://arxiv.org/pdf/2001.10256.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(HTML_DUMP_FN, mode=\"r:gz\") as tar:\n",
    "    # we're just going to look at the first block of articles -- each one is a maximum of 10GB\n",
    "    html_fn = tar.next()\n",
    "    print(f'We will be working with {html_fn.name} ({html_fn.size / 1000000000:0.3f} GB).')\n",
    "    # extract the first file\n",
    "    cum = [0.0, 0.0, 0.0, 0.0]\n",
    "    with tar.extractfile(html_fn) as fin:\n",
    "        a = 0\n",
    "        for line in fin:\n",
    "            article = json.loads(line)\n",
    "            wiki_result = wikitext(article['article_body']['wikitext'])\n",
    "            html_result = html(article['article_body']['html'])\n",
    "            wiki_feature = output_wikitext(wt)\n",
    "            html_feature = output_html(soup)\n",
    "            #CUmulative calculates the average in differences of features\n",
    "            for i in range(0, 4):\n",
    "                cum[i] += abs(wiki_feature[i] - html_feature[i])\n",
    "            a = a + 1\n",
    "            if a > 100:\n",
    "                break\n",
    "    print(\"Averages:: \")\n",
    "    print(\"for feature tags: \", cum[0]/100)\n",
    "    print(\"for feature external links: \", cum[1]/100)\n",
    "    print(\"for feature comments: \", cum[2]/100)\n",
    "    print(\"for feature sections: \", cum[3]/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: text comparison analysis\n",
    "# For example, are there certain words that show up more frequently in the HTML versions but not the wikitext? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this project, `stemming` has been used to analyse text between html and wikitext. in nlp, stemming refers to a process of reducing word to its word stem and prefixes to the root. This helps analyse the data better and felicitates easy comparison between two pieces of text. Bar graphs have been generated using the stemming results between wikitext and html.  \n",
    "  \n",
    "  To conduct more complex analysis, one can also use `lemmatisation`. Lemmatisation helps us understand the vocabulary and the morphological structure of the words used in the HTML and wikitext document. By comparing them, we will be able to analyse the base dictionaries involved in encoding both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /srv/paws/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /srv/paws/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /srv/paws/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /srv/paws/lib/python3.8/site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /srv/paws/lib/python3.8/site-packages (from nltk) (2022.3.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stemming and lemitization (nltk)\n",
    "# frequency plot for each and compare\n",
    "wt = mw.parse(article['article_body']['wikitext'])\n",
    "wikitext_text = wt.strip_code()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'{': 34, 'multiple': 1, 'issues|': 1, 'more': 1, 'footnotes|date=October': 1, '2014': 3, '}': 34, 'BLP': 1, 'sources|date=February': 1, '2015': 1, 'Autobiography|date=October': 1, '2019': 1, 'Infobox': 1, 'person': 1, '|': 15, 'name': 1, '=': 14, 'Jashim': 4, 'Salam': 5, 'image': 1, 'Salam.jpg': 1, 'alt': 1, 'caption': 1, 'birth_place': 1, 'Chittagong': 1, ',': 35, 'Bangladesh': 3, 'death_date': 1, '<': 15, '!': 2, '--': 2, 'Death': 1, 'date': 1, 'and': 7, 'age|YYYY|MM|DD|YYYY|MM|DD': 1, '>': 15, 'death_place': 1, 'nationality': 1, 'Bangladeshi': 4, 'education': 1, '[': 23, 'Ateneo': 3, 'de': 3, 'Manila': 3, 'University': 3, ']': 23, 'Pathshala|Pathshala': 1, 'South': 1, 'Asian': 4, 'Media': 1, 'Academy': 1, 'occupation': 1, 'Photojournalist': 1, 'years_active': 1, '2004-': 1, 'present': 1, 'known_for': 1, 'Documentary': 1, 'photography': 6, 'notable_works': 1, 'website': 1, 'URL|jashimsalam.com/': 1, \"''\": 16, \"'Jashim\": 1, \"'\": 1, '(': 5, 'born': 1, '1978': 1, ')': 5, 'is': 1, 'a': 3, 'documentary': 1, 'photographer': 1, 'photojournalist': 1, '.': 9, '==Biography==': 1, 'graduated': 1, 'in': 11, 'from': 1, 'Pathshala': 1, 'the': 9, 'south': 1, 'institute': 1, 'of': 10, 'media': 1, 'academy': 1, 'He': 4, 'has': 4, 'studied': 1, 'post': 1, 'Graduation': 1, 'Diploma': 1, 'Visual': 1, 'Journalism': 2, 'through': 1, 'scholarship': 2, 'program': 1, 'World': 2, 'Press': 3, 'Photo': 3, 'at': 2, 'Konrad': 1, 'Adenauer': 1, 'Center': 1, 'for': 3, 'ACFJ': 1, 'Philippines': 1, 'been': 3, 'teaching': 1, 'mentoring': 1, 'workshops': 1, 'seminars': 1, 'aspiring': 1, 'young': 1, 'photographers': 2, 'regularly': 1, 'also': 1, 'on': 1, 'jury': 1, 'boards': 1, 'numerous': 1, 'contests': 1, 'Social': 1, 'photo-documentary': 1, 'portraits': 1, 'are': 1, 'among': 1, 'his': 2, 'interests': 1, 'started': 1, 'photographic': 1, 'career': 1, 'with': 1, 'DrikNEWS': 1, 'agency': 3, 'worked': 1, 'New': 2, 'Age': 1, 'an': 1, 'English': 1, 'daily': 1, 'Drik': 1, 'picture': 1, 'Majority': 1, 'world': 1, 'Nur': 1, 'photo': 1, 'Corbis': 1, 'images': 1, 'His': 1, 'work': 1, 'featured': 1, 'exhibitions': 1, 'screening': 1, 'worldwide': 1, 'including': 1, 'Visa': 1, 'Pour': 1, \"l'Image\": 1, 'Perpignan': 1, 'The': 5, 'Photoville': 1, 'festival': 1, 'York': 1, 'Berlin': 1, 'Parliament': 1, 'Germany': 1, 'Atrium': 1, 'Town': 1, 'Hall': 1, 'Hague': 1, 'Netherlands': 1, 'Maison': 1, 'familiale': 1, 'Pro': 1, 'Juventute': 1, 'Geneva': 1, 'Getty': 1, 'Images': 1, 'Gallery': 2, 'London': 1, 'French': 2, 'Alliance': 1, 'Foundation': 1, 'Paris': 1, 'France': 1, '==Awards==': 1, '*': 6, '2nd': 1, 'spotlight': 1, 'fotovisura': 1, 'grant': 1, '2013': 2, 'ref': 7, 'name=': 7, 'Fotovisura': 1, 'cite': 7, 'web|url=http': 2, ':': 21, '//www.hfs.si/2/category/grants/1.html': 1, '|title=': 2, 'FotoVisura': 1, 'Grant': 1, 'Results': 1, '|author=Fotovisura': 1, 'USA': 1, '|date=': 4, 'July': 2, '5': 1, '/ref': 7, '1st': 2, 'prize': 2, 'Gold': 1, '3rd': 2, 'contest': 1, '2012': 3, 'Asianewsphoto': 1, 'web': 3, '|url=http': 2, '//www.asianewsphoto.com/Web_ENG/AfficheList1.aspx': 2, '?': 2, 'AfficheID=76': 2, '|title=The': 1, 'Asia': 1, 'Contest': 1, 'announced': 1, 'Beijing': 1, '|author=Asianewsphoto': 1, '|date=2012-09-20': 1, '|access-date=2014-11-01': 2, '|archive-url=https': 2, '//web.archive.org/web/20171001211025/http': 1, '|archive-date=2017-10-01': 1, '|url-status=dead': 2, 'Emirates': 3, 'award': 1, '2011': 3, 'Photography': 4, '|url=': 1, 'http': 2, '//emiratesphotocompetition.ae/en/6th-session-2012': 2, '6th': 1, 'Session': 1, '|author=': 4, 'Competition': 1, 'UAE': 1, '26': 1, '|access-date=': 1, '1': 1, 'November': 1, '|archive-url=': 1, 'https': 1, '//web.archive.org/web/20140228193807/http': 1, '|archive-date=': 1, '28': 1, 'February': 1, '|url-status=': 1, 'dead': 1, 'Ian': 2, 'Parry': 3, 'Commended': 1, 'IanParry2': 1, '//www.ianparry.org/photographers/2011-jashim-salam-commended/': 1, '|title=Water': 1, 'England': 3, 'ianparry1': 1, '//www.ianparry.org/alumni/': 2, '|title=Alumni': 1, '|author=Ian': 1, 'Scholarship': 1, '|year=2011': 1, '//web.archive.org/web/20190201161030/http': 1, '|archive-date=2019-02-01': 1, 'OpenWalls': 2, 'Arles': 2, '2020': 3, 'Winner': 2, 'OpenWallsArles': 1, 'web|url=https': 2, '//www.bjp-online.com/2020/09/something-has-to-change-portraits-of-growth-from-openwalls-arles-2020/': 1, '|title=InÃªs': 1, 'd': 1, 'â': 1, 'Orey': 1, 'View': 1, 'Gallery-': 1, 'page': 1, 'no': 1, '6': 1, '19': 1, 'British': 1, 'Journal': 1, '|date=2020-09-03': 1, 'Eye': 2, 'Photograpy': 1, '//loeildelaphotographie.com/en/arles-2020-open-walls-some-portraits-dfg/': 1, '|title=Arles': 1, 'Open': 1, 'Walls': 1, 'Some': 1, 'Portraits': 1, 'ultimate': 1, 'digital': 1, 'magazine': 1, '==References==': 1, 'reflist|2': 1, '==External': 1, 'links==': 1, '//www.jashimsalam.com': 1, 'Website': 1, 'authority': 1, 'control': 1, 'DEFAULTSORT': 1, 'Category:1978': 1, 'births': 1, 'Category': 4, 'Living': 1, 'people': 1, 'journalists': 1, 'alumni': 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = article['article_body']['wikitext']\n",
    "\n",
    "words = word_tokenize(sentences)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "a = {}\n",
    "for w in words:\n",
    "    if w not in a:\n",
    "        a[w] = 1\n",
    "    else:\n",
    "        a[w] += 1\n",
    "# Stemming has been performed on the words and the dictionary has been printed\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(*zip(*D.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: time comparison,\n",
    "# How fast is each method for extracting features,\n",
    "# Is working with the larger but already parsed HTML faster than parsing the more compact wikitext with mwparserfromhell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be working with enwiki_0.ndjson (10.000 GB).\n",
      "CPU times: user 1.32 s, sys: 15.8 ms, total: 1.33 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tarfile.open(HTML_DUMP_FN, mode=\"r:gz\") as tar:\n",
    "    # we're just going to look at the first block of articles -- each one is a maximum of 10GB\n",
    "    html_fn = tar.next()\n",
    "    print(f'We will be working with {html_fn.name} ({html_fn.size / 1000000000:0.3f} GB).')\n",
    "    # extract the first file\n",
    "    with tar.extractfile(html_fn) as fin:\n",
    "        a = 0\n",
    "        for line in fin:\n",
    "            article = json.loads(line)\n",
    "            wiki_result = wikitext(article['article_body']['wikitext'])\n",
    "            a = a + 1\n",
    "            if a > 100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be working with enwiki_0.ndjson (10.000 GB).\n",
      "CPU times: user 2.41 s, sys: 31.7 ms, total: 2.44 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tarfile.open(HTML_DUMP_FN, mode=\"r:gz\") as tar:\n",
    "    # we're just going to look at the first block of articles -- each one is a maximum of 10GB\n",
    "    html_fn = tar.next()\n",
    "    print(f'We will be working with {html_fn.name} ({html_fn.size / 1000000000:0.3f} GB).')\n",
    "    # extract the first file\n",
    "    with tar.extractfile(html_fn) as fin:\n",
    "        a = 0\n",
    "        for line in fin:\n",
    "            article = json.loads(line)\n",
    "            html_result = html(article['article_body']['html'])\n",
    "            # move to the next file\n",
    "            a = a + 1\n",
    "            # if 100 articles have been looped through, do not go to the next article\n",
    "            if a > 100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Comparison\n",
    "\n",
    "As evident from the above two cells and their respective time output, the more compact wikitext with mwparserfromhell is faster to work with as compared to the large but already parsed HTML.\n",
    "\n",
    "The time taken to extract features from wikitext with mwparserfromhell is the following :\n",
    "  \n",
    "  `CPU times: user 1.32 s, sys: 15.8 ms, total: 1.33 s`  \n",
    "  `Wall time: 1.37 s`\n",
    "\n",
    "The time taken to extract features from already parsed html is the following :\n",
    "\n",
    "  \n",
    "  `CPU times: user 2.41 s, sys: 31.7 ms, total: 2.44 s`  \n",
    "  `Wall time: 2.48 s`\n",
    "\n",
    "The following is the case since wiktexts' emphasis is more on the content rather than the presentation and the formatting. Therefore, this is a medium which requires minimal efforts towards beautifying the content.  \n",
    "On the other hand, HTML involves the usage of tags for fonts, fontstyle, colors, and others which focuses on the formatting and hence the presentation. Thus, along with the content there are numerous tags with their attributes which makes HTML larger and more complex than wikitext.  \n",
    "In addition to this, as already mentioned, through the process of parsing, the templates and modules expand in the output, which is the HTML, leading to more information and content in HTML as compared to wikitext. Wikitext is an approximation of the complete information.  \n",
    "  \n",
    "  Below are two bar plots of CPU time and Wall time respectively. These graphs show the graphical difference in time taken by the wikitext body and the HTML body. It is clear from the graphs as well that it takes longer to work with HTML than working with wikitext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'CPU time')"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CPU_times = [1.32, 2.41]\n",
    "Wall_times = [1.37, 2.48]\n",
    "x_axis = [\"wikitext\", \"html\"]\n",
    "plt.bar(x_axis, height = CPU_times, color = \"maroon\")\n",
    "plt.title(\"CPU time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Wall time')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x_axis, height = Wall_times, color = \"orange\")\n",
    "plt.title(\"Wall time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
